{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3314c75-cad7-44b8-b668-885b31332cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7abe8c9-18e8-48e6-bf34-1bee172b9631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcp-starter'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('./.env')\n",
    "os.environ.get('PINECONE_ENV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34133356-0f4d-4362-b46a-a72a708acb63",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (0.1.15)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.13 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from langchain-openai) (0.2.16)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.32.0 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from langchain-openai) (1.35.13)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.13->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.13->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.13->langchain-openai) (0.1.85)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.13->langchain-openai) (24.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.13->langchain-openai) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.13->langchain-openai) (8.5.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.12.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain-openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.13->langchain-openai) (2.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.13->langchain-openai) (3.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.3.0,>=0.2.13->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.3.0,>=0.2.13->langchain-openai) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jackson\\workshop-llm-2\\env_llm\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.32.0->langchain-openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ffba0e5-a651-4510-aea1-eb6767f9b8cd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jackson\\workshop-llm-2\\env_llm\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "362b428f-b75c-4a6f-867b-81170c6a2cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import(\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ff8788c-5d88-4063-90ba-74bd0e61d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Carregando {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f'Carregando {file}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    else:\n",
    "        print('Formato n√£o suportado!')\n",
    "        return None\n",
    "\n",
    "    data = loader.load()\n",
    "    return data\n",
    "\n",
    "# Wikipedia Loader\n",
    "def load_from_wikipedia(query, lang='pt', load_max_docs=2):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "123ad7f3-e404-49d5-bd79-8b01b617af64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=1000):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cf02515-6b0e-4311-a49e-20ee979616b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total de tokens: {total_tokens}')\n",
    "    print(f'Custo de Embedding em USD: {total_tokens / 1000 * 0.0001:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "978eb3d2-6a90-4199-9552-4ccb82dd4ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando docs/CLT.pdf\n"
     ]
    }
   ],
   "source": [
    "data = load_document('docs/CLT.pdf')\n",
    "chunks = chunk_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdae9fd2-97c5-48e5-a896-0f986ad6bbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de tokens: 252697\n",
      "Custo de Embedding em USD: 0.025270\n"
     ]
    }
   ],
   "source": [
    "embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec394d41-d93a-45c7-8150-67a7fbfd4aa4",
   "metadata": {},
   "source": [
    "## insert embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6d11825-87cd-4fd4-8c9b-633af2a6325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd940c2f-a076-4196-b6bd-339ae3b0e1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = embeddings.embed_query(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04a71329-9f00-4e73-ad19-38a4905a8f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "pc = Pinecone(api_key=\"xxxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37ea984b-c1d2-44a6-b688-e090f11acd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"linuxtips\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e08b7247-1bd4-42c4-9191-8755bb9b6195",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=1536, # Replace with your model dimensions\n",
    "    metric=\"cosine\", # Replace with your model metric\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa45551c-93d0-4a73-b070-b0606f151f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore.from_documents(chunks, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cbfd35-1011-4c96-ab0a-3105622f0f1f",
   "metadata": {},
   "source": [
    "## Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dcaeb56-d604-4106-8d75-ce5596231cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(vector_store, q):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    #from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5, max_tokens=1024)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=retriever)\n",
    "\n",
    "    answer = chain.run(q)\n",
    "    return answer\n",
    "\n",
    "def ask_with_memory(vector_store, question, chat_history=[]):\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    #from langchain.chat_models import ChatOpenAI\n",
    "    \n",
    "    llm = ChatOpenAI(temperature=1)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "    \n",
    "    crc = ConversationalRetrievalChain.from_llm(llm, retriever)\n",
    "    result = crc({'question': question, 'chat_history': chat_history})\n",
    "    chat_history.append((question, result['answer']))\n",
    "    \n",
    "    return result, chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db00d292-e0aa-4be9-9594-86b77ac7e956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jackson\\workshop-llm-2\\env_llm\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Consolida√ß√£o das Leis do Trabalho (CLT) foi criada em 1943.\n"
     ]
    }
   ],
   "source": [
    "q = 'em que ano a clt foi criada?'\n",
    "answer = get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82e89752-d3a9-4b30-b2c5-1a4abed96600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digite sair para encerrar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta: #1:  Como funciona as f√©rias ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resposta: As f√©rias s√£o um per√≠odo de descanso remunerado concedido ao trabalhador ap√≥s um per√≠odo de trabalho cont√≠nuo. De acordo com a legisla√ß√£o trabalhista, a concess√£o das f√©rias deve ser comunicada por escrito ao empregado com anteced√™ncia m√≠nima de 30 dias. Durante as f√©rias, o empregado n√£o pode prestar servi√ßos a outro empregador, a n√£o ser que tenha um contrato de trabalho regularmente mantido. Se as f√©rias forem concedidas ap√≥s o prazo estabelecido, o empregador deve pagar em dobro a remunera√ß√£o correspondente. Al√©m disso, existem regras espec√≠ficas para f√©rias coletivas e para empregados menores de 18 anos.\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta: #2:  sair\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encerrando...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "i = 1\n",
    "print('Digite sair para encerrar.')\n",
    "while True:\n",
    "    q = input(f'Pergunta: #{i}: ')\n",
    "    i = i+1\n",
    "    if q.lower() in ['sair']:\n",
    "        print('Encerrando...')\n",
    "        time.sleep(2)\n",
    "        break\n",
    "\n",
    "    answer = get_answer(vector_store, q)\n",
    "    print(f'\\nResposta: {answer}')\n",
    "    print(f'\\n {\"-\" * 50} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b32b030-82ae-42cf-b261-03ce05f1cb19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
